# vLLM Model Configuration
# Copy this to .env and customize

# ==============================================
# Model Selection
# ==============================================
# Choose a model that fits your GPU VRAM:
#
# For 16GB VRAM (RTX 4090 Laptop):
#   - mistralai/Mistral-7B-Instruct-v0.3 (7B, fast)
#   - Qwen/Qwen2.5-7B-Instruct (7B, multilingual)
#   - meta-llama/Llama-3.2-3B-Instruct (3B, very fast)
#   - microsoft/Phi-3-mini-4k-instruct (3.8B, efficient)
#
# For 24GB+ VRAM:
#   - Qwen/Qwen2.5-14B-Instruct (14B)
#   - meta-llama/Llama-3.1-8B-Instruct (8B)
#
# ==============================================

# HuggingFace model ID
VLLM_MODEL=mistralai/Mistral-7B-Instruct-v0.3

# Model name exposed via API (for OpenAI-compatible calls)
VLLM_SERVED_NAME=mistral-7b

# Maximum context length (reduce if running out of memory)
VLLM_MAX_MODEL_LEN=8192

# GPU memory utilization (0.0-1.0, lower = more headroom)
VLLM_GPU_MEMORY_UTIL=0.90

# Data type (auto, float16, bfloat16)
VLLM_DTYPE=auto

# ==============================================
# Optional Settings
# ==============================================

# HuggingFace token (required for gated models like Llama)
# Get yours at: https://huggingface.co/settings/tokens
HF_TOKEN=

# Tensor parallelism (for multi-GPU setups)
# VLLM_TENSOR_PARALLEL=1

# Quantization (for running larger models in less VRAM)
# Options: awq, gptq, squeezellm
# VLLM_QUANTIZATION=
